{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3df017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ftfy\n",
    "import unidecode\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8159710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK dependencies\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c18742",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"Dataset/completeSpamAssassin.csv\",\n",
    "    \"Dataset/emails.csv\",\n",
    "    \"Dataset/enronSpamSubset.csv\",\n",
    "    \"Dataset/lingSpam.csv\",\n",
    "    \"Dataset/spam mail.csv\"\n",
    "]\n",
    "dfs = [pd.read_csv(p, encoding='utf-8') for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9a82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dfs):\n",
    "    unnamed = [c for c in df.columns if c.startswith(\"Unnamed\")]\n",
    "    if unnamed:\n",
    "        df.drop(columns=unnamed, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "219b78a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_10060\\1522610312.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"Label\"] = df[\"Label\"].replace({\"ham\": 0, \"spam\": 1, \"not spam\": 0}).astype(int)\n"
     ]
    }
   ],
   "source": [
    "rename_map = {\n",
    "    \"text\": \"Body\",\n",
    "    \"Message\": \"Body\",\n",
    "    \"Masseges\": \"Body\",\n",
    "    \"Category\": \"Label\",\n",
    "    \"spam\": \"Label\",\n",
    "    \"label\": \"Label\"\n",
    "}\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    df.rename(columns={col: rename_map.get(col, col) for col in df.columns}, inplace=True)\n",
    "    if \"Label\" in df.columns:\n",
    "        df[\"Label\"] = df[\"Label\"].replace({\"ham\": 0, \"spam\": 1, \"not spam\": 0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf51103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "def basic_clean(text):\n",
    "    text = str(text)\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r\"Subject:\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" URL \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" EMAIL \", text)\n",
    "    text = re.sub(r\"\\d+\", \" NUMBER \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "for df in dfs:\n",
    "    if \"Body\" in df.columns:\n",
    "        df[\"Body\"] = df[\"Body\"].astype(str).apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ebb8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned df1: removed 906 duplicates/NaN\n",
      "âœ… Cleaned df2: removed 230 duplicates/NaN\n",
      "âœ… Cleaned df3: removed 617 duplicates/NaN\n",
      "âœ… Cleaned df4: removed 17 duplicates/NaN\n",
      "âœ… Cleaned df5: removed 476 duplicates/NaN\n"
     ]
    }
   ],
   "source": [
    "# 5ï¸âƒ£ Remove duplicates and empty entries\n",
    "for i, df in enumerate(dfs):\n",
    "    before = len(df)\n",
    "    df.drop_duplicates(subset=[\"Body\"], inplace=True)\n",
    "    df.dropna(subset=[\"Body\"], inplace=True)\n",
    "    print(f\"âœ… Cleaned df{i+1}: removed {before - len(df)} duplicates/NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415a7747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§© Combined dataset shape: (27648, 2)\n"
     ]
    }
   ],
   "source": [
    "# 6ï¸âƒ£ Merge datasets and shuffle\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df = combined_df.dropna(subset=[\"Body\", \"Label\"])\n",
    "combined_df = combined_df[combined_df[\"Body\"].str.len() > 10]  # remove tiny texts\n",
    "combined_df = shuffle(combined_df, random_state=42).reset_index(drop=True)\n",
    "print(f\"ğŸ§© Combined dataset shape: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90c4e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ English-only messages: (26849, 3)\n"
     ]
    }
   ],
   "source": [
    "# 7ï¸âƒ£ Language filter (optional but useful)\n",
    "def safe_detect(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "combined_df[\"lang\"] = combined_df[\"Body\"].apply(safe_detect)\n",
    "combined_df = combined_df[combined_df[\"lang\"] == \"en\"]\n",
    "\n",
    "print(f\"ğŸŒ English-only messages: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4b39b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8ï¸âƒ£ Lemmatization + stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def advanced_clean(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "combined_df[\"Clean_Body\"] = combined_df[\"Body\"].apply(advanced_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "200bbaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Spam: 8217, Ham: 18632\n",
      "âœ… Final balanced shape: (26849, 4)\n"
     ]
    }
   ],
   "source": [
    "# 9ï¸âƒ£ Handle class imbalance\n",
    "spam_count = combined_df[\"Label\"].sum()\n",
    "ham_count = len(combined_df) - spam_count\n",
    "print(f\"ğŸ“Š Spam: {spam_count}, Ham: {ham_count}\")\n",
    "\n",
    "# Option 1: Use undersampling if too imbalanced\n",
    "if spam_count < ham_count / 3:\n",
    "    ham_df = combined_df[combined_df[\"Label\"] == 0].sample(spam_count * 2, random_state=42)\n",
    "    spam_df = combined_df[combined_df[\"Label\"] == 1]\n",
    "    combined_df = pd.concat([ham_df, spam_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Final balanced shape: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e2a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved cleaned dataset â†’ Dataset/cleaned_spam_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”š Save the cleaned dataset\n",
    "combined_df.to_csv(\"Dataset/cleaned_spam_dataset.csv\", index=False)\n",
    "print(\"ğŸ’¾ Saved cleaned dataset â†’ Dataset/cleaned_spam_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
