{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "cb985342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8d39bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Datasets ---\n",
    "df1 = pd.read_csv(\"Dataset/completeSpamAssassin.csv\")\n",
    "df2 = pd.read_csv(\"Dataset/emails.csv\")\n",
    "df3 = pd.read_csv(\"Dataset/enronSpamSubset.csv\")\n",
    "df4 = pd.read_csv(\"Dataset/lingSpam.csv\")\n",
    "df5 = pd.read_csv(\"Dataset/spam mail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c4081002",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1591c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dropped ['Unnamed: 0'] from df1\n",
      "â„¹ï¸ No unnamed columns found in df2\n",
      "âœ… Dropped ['Unnamed: 0.1', 'Unnamed: 0'] from df3\n",
      "âœ… Dropped ['Unnamed: 0'] from df4\n",
      "â„¹ï¸ No unnamed columns found in df5\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Drop unnamed columns safely ---\n",
    "for idx, df in enumerate(dfs, start=1):\n",
    "    unnamed_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "    if unnamed_cols:\n",
    "        df.drop(columns=unnamed_cols, inplace=True)\n",
    "        print(f\"âœ… Dropped {unnamed_cols} from df{idx}\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ No unnamed columns found in df{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8fc20252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Standardize column names ---\n",
    "df2.rename(columns={'text': 'Body', 'spam': 'Label'}, inplace=True)\n",
    "df5.rename(columns={'Category': 'Label', 'Masseges': 'Body'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "09da80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_15792\\1075495117.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df5[\"Label\"] = df5[\"Label\"].replace({\"ham\": 0, \"spam\": 1})\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Convert text labels to numeric ---\n",
    "df5[\"Label\"] = df5[\"Label\"].replace({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5c15b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Clean â€œSubject:â€ prefix if exists ---\n",
    "for df in [df3, df4]:\n",
    "    if \"Body\" in df.columns:\n",
    "        df[\"Body\"] = df[\"Body\"].astype(str).str.replace(\"Subject:\", \"\", regex=False).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "db78e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Keep only Body + Label columns where available ---\n",
    "for i, df in enumerate(dfs):\n",
    "    if all(col in df.columns for col in [\"Body\", \"Label\"]):\n",
    "        dfs[i] = df[[\"Body\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7c29b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined dataset shape: (29951, 2)\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Merge all into one big DataFrame ---\n",
    "dfs = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"âœ… Combined dataset shape: {dfs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "4d7c5cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"Label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cab9cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… After cleaning: (28434, 2)\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Drop missing & duplicate rows ---\n",
    "dfs.dropna(inplace=True)\n",
    "dfs.drop_duplicates(inplace=True)\n",
    "print(f\"âœ… After cleaning: {dfs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d75a8d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nikhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 9. Text preprocessing ---\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "0a5e9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "162f31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)       # remove non-letter characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    # remove extra spaces\n",
    "    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9b240378",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Body'] = dfs['Body'].apply(clean_text)\n",
    "dfs = dfs[dfs['Body'].str.len() > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "da0d122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Word count stats:\n",
      " count    2.839700e+04\n",
      "mean     1.854568e+02\n",
      "std      8.703130e+03\n",
      "min      1.000000e+00\n",
      "25%      2.100000e+01\n",
      "50%      6.100000e+01\n",
      "75%      1.410000e+02\n",
      "max      1.465726e+06\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "ðŸ“ˆ Label distribution:\n",
      " Label\n",
      "0    0.697996\n",
      "1    0.302004\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Quality checks ---\n",
    "dfs['word_count'] = dfs['Body'].apply(lambda x: len(x.split()))\n",
    "print(\"\\nðŸ“Š Word count stats:\\n\", dfs['word_count'].describe())\n",
    "print(\"\\nðŸ“ˆ Label distribution:\\n\", dfs['Label'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "909aab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final dataset sample:\n",
      "                                                Body  Label  word_count\n",
      "0  save life insurance spend life quote saving en...      1          95\n",
      "1  fight risk cancer slim guaranteed lose lb day ...      1          52\n",
      "2  fight risk cancer slim guaranteed lose lb day ...      1          37\n",
      "3  adult club offer free membership instant acces...      1         207\n",
      "4  thought might like slim guaranteed lose lb day...      1          44\n",
      "\n",
      "âœ… Final dataset shape: (28397, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- 11. Final check ---\n",
    "print(\"\\nâœ… Final dataset sample:\")\n",
    "print(dfs.head())\n",
    "print(f\"\\nâœ… Final dataset shape: {dfs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a71ba9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved cleaned dataset as 'cleaned_spam_dataset.csv' with 28397 rows.\n"
     ]
    }
   ],
   "source": [
    "# --- 12. Save cleaned dataset ---\n",
    "dfs.to_csv(\"cleaned_spam_dataset.csv\", index=False, encoding='utf-8')\n",
    "print(f\"âœ… Saved cleaned dataset as 'cleaned_spam_dataset.csv' with {dfs.shape[0]} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8e5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
